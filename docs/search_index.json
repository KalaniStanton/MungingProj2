[["index.html", "An analysis of Cryptocurrency Preface", " An analysis of Cryptocurrency Timothy McCormack, Carolyn Herrera, Wei Chun Tien, 2020-11-09 Preface "],["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction "],["data-import-and-clean.html", "Chapter 2 Data Import and Clean 2.1 Import Data 2.2 Data Collection 2.3 Clean 2.4 Comments 2.5 Export 2.6 Works Cited", " Chapter 2 Data Import and Clean library(tidyverse) library(tidytext) library(textclean) library(dplyr) library(stringr) library(knitr) library(wordcloud) library(kableExtra) library(DT) library(tidygraph) library(ggraph) library(tm) knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = TRUE) # set to TRUE to run this on only one reference file debugging &lt;- FALSE # this will expect the file or files to be in a subdirectory with the following name refsource &lt;- &quot;MungingProj2&quot; dataDir &lt;- &quot;Proj2Data&quot; workingDir &lt;- refsource # prefixes for all File reads and writes # titles for tables titletext &lt;- &quot;RedditCrypto&quot; srs = c(&quot;CryptoCurrency&quot;,&quot;CryptoMarkets&quot;) `%notin%` &lt;- Negate(`%in%`) 2.1 Import Data subm_fnames &lt;- list.files(dataDir, pattern = &quot;*_submissions.csv&quot;, full.names = TRUE) comm_fnames &lt;- list.files(dataDir, pattern = &quot;*_comments.csv&quot;, full.names = TRUE) subr_fnames &lt;- list.files(dataDir, pattern = &quot;*_subreddit.csv&quot;, full.names = TRUE) for (i in 1:length(subm_fnames)) assign(srs[i], read.csv(subm_fnames[i])) SubmData&lt;- rbind(CryptoCurrency, CryptoMarkets) for (i in 1:length(comm_fnames)) assign(srs[i], read.csv(comm_fnames[i])) CommData&lt;- rbind(CryptoCurrency, CryptoMarkets) for (i in 1:length(subr_fnames)) assign(srs[i], read.csv(subr_fnames[i])) SubrData&lt;- rbind(CryptoCurrency, CryptoMarkets) 2.1.1 Fix Names srs &lt;- unique(SubmData$subreddit) SubmData &lt;- SubmData %&gt;% mutate( subreddit = case_when( .$subreddit == srs[1] ~ &quot;r/CryptoCurrency&quot;, .$subreddit == srs[2] ~ &quot;r/CryptoMarkets&quot;) ) %&gt;% ungroup() 2.2 Data Collection submnums &lt;- table(SubmData$subreddit) SubmNums &lt;- as.data.frame(submnums, .name_repair = &quot;minimal&quot;) colnames(SubmNums)[1] &lt;- &quot;Subreddit&quot; ggplot(SubmNums, aes(x = Subreddit, y = Freq, fill = Subreddit)) + geom_bar(stat = &quot;identity&quot;) + scale_y_continuous(name=&quot;# of Submissions by Subreddit&quot;, labels = scales::comma) c &lt;- CommData commnums &lt;- table(c$subreddit) CommNums &lt;- as.data.frame(commnums, .name_repair = &quot;minimal&quot;) colnames(CommNums)[1] &lt;- &quot;Subreddit&quot; ggplot(CommNums, aes(x = Subreddit, y = Freq, fill = Subreddit)) + geom_bar(stat = &quot;identity&quot;) + scale_y_continuous(name=&quot;# of Comments by Subreddit (from 300 posts each)&quot;, labels = scales::comma) kable(SubrData) %&gt;% kable_styling(&quot;striped&quot;, full_width = F) X title subscribers created public_description 0 Cryptocurrency News &amp; Discussion 1117855 2013-03-11 17:51:50 The official source for CryptoCurrency News, Discussion &amp; Analysis. 0 r/CryptoMarkets 236835 2013-11-12 18:50:17 FOREX community for cryptocurrencies. Tags: mt gox bitcoin, long term potential, open source exchange, low inflation rate, demand and price, technical analysis, fundamentals, Bitcoin, Ethereum, Litecoin, Monero, Dash, Augur, token, volume, oscillator, RSI, stochastic, trend, sentiment, strategy, scam, coin, coinmarketcap, altcoin, Peercoin, script, blockchain, PoW, PoS, Proof of Work, Submissions from each subreddit: Comments in each subreddit: 2.3 Clean To ensure that our data accurately represent activity within the communitas, we want to ensure that each observation is a unique instance of engagement. Reposts tend to be common on reddit, so using a distinct() function on the text column will remove any duplicate posts. Additionally, any posts that are removed from the subreddits return an NA value in user column, thus we can remove deleted comments by filtering out all non user == NA. #Clean Encoding(SubmData$text) &lt;- &quot;UTF-8&quot; Submissions &lt;- SubmData %&gt;% group_by(user) %&gt;% filter(!is.na(user)) %&gt;% # Take out deleted comments ungroup() %&gt;% distinct(text, .keep_all = TRUE) #remove duplicate submissions paste(&quot;Removed&quot;, nrow(SubmData) - nrow(Submissions),&quot;submissions.&quot;) ## [1] &quot;Removed 14 submissions.&quot; 2.3.1 Submissions The analysis of langauge begins by quantifying the presence of words in each Subm, through the process of tokenization. Tokens are discrete strings of words or characters that can be isolated as n-grams; with n pertaining to the number of words in each token. Tokens are pulled from the body of text that is most informative for the purposes of analysis. For submissions, the informative text is the title of the submission, which contains information on topics; whereas, for comments, the informative text is the comment itself. data(stop_words) SubmissionTkns &lt;- Submissions %&gt;% group_by(subreddit) %&gt;% unnest_tokens(word, text) %&gt;% ungroup() #G2Subm &lt;- Submissions %&gt;% # group_by(subreddit) %&gt;% # unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2, n_min = #2) %&gt;% #repeat above as bigram # ungroup() As part of the tokenization process for submissions, we remove stop words (e.g. and, a, the) because we are interested in using our tokens to identify prevalent topics of discussion and attitudes in the religiously affiliated social clusters. In contrast, stop words are not removed from text data in comments because they are useful in the analysis of linguistic sentiment and behavior. #### 1-grams (Clean) #Create object for numbers, so that we can remove them from the data nums &lt;- SubmissionTkns %&gt;% filter(str_detect(word, &quot;^[0-9]&quot;)) %&gt;% select(word) %&gt;% unique() #Source: https://richpauloo.github.io/2017-12-29-Using-tidytext-to-make-word-clouds/ SubmissionTkns &lt;- SubmissionTkns %&gt;% anti_join(stop_words, by = &quot;word&quot;) %&gt;% anti_join(nums, by = &quot;word&quot;) %&gt;% filter(!grepl(&quot;_&quot;, .$word)) G1Subm &lt;- SubmissionTkns 2.3.1.1 2-grams (Clean) 2-gram code is not used in analysis due to unsolved cleaning error. #G2Subm_separated &lt;- G2Subm %&gt;% #Separate bigrams to remove stopwords and transitional phrases # separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) #run boolean functions for cleaning on `word1` and `word2` #G2Subm_filtered &lt;- G2Subm_separated %&gt;% # filter(!word1 %in% stop_words$word) %&gt;% #remove stop words # filter(!word2 %in% stop_words$word) %&gt;% # filter(!word1 == &quot;NA&quot;) %&gt;% #remove anomylous strings # filter(!word2 == &quot;NA&quot;) %&gt;% # filter(!word1 %in% nums) %&gt;% #remove numbers # filter(!word2 %in% nums) #G2Subm &lt;- G2Subm_filtered %&gt;% # unite(bigram, word1, word2, sep = &quot; &quot;)#reunify cleaned word columns #G2Subm &lt;- G2Subm[!grepl(&quot;_&quot;, G2Subm$bigram),] #rm(G2Subm_separated) #rm(G2Subm_filtered) 2.3.2 Count Following tokenization, the presence of each n-gram is tallied within each communitas. The tallies are then sorted to show what the most frequently occuring words are in each category. G1SubmCount &lt;- G1Subm %&gt;% group_by(subreddit) %&gt;% #group words by affiliation label count(word, sort = FALSE) %&gt;% #count and create column &#39;n&#39; ungroup() #G2SubmCount &lt;- G2Subm %&gt;% #repeat for bigrams # group_by(subreddit) %&gt;% # count(bigram, sort = FALSE) %&gt;% # ungroup() rm(G1Subm) #rm(G2Subm) 2.4 Comments The cleaning and tokenization process for comments is similar to that of submissions; however, a few extra steps must be taken considering the casual and conversational nature of reddit comments. NEED TO DO: Additionally, the number of comments on each submission varies, so it will be important to standardize word counts across subreddits.. 2.4.1 Clean Encoding(CommData$text) &lt;- &quot;UTF-8&quot; Comments &lt;- CommData %&gt;% group_by(user) %&gt;% filter(!is.na(user)) %&gt;% # Take out deleted comments ungroup() %&gt;% distinct(text, .keep_all = TRUE) #remove duplicate comments paste(&quot;Removed&quot;, nrow(CommData) - nrow(Comments),&quot;comments&quot;) ## [1] &quot;Removed 12771 comments&quot; url_regex &lt;- &quot;http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+&quot; Comments$text &lt;-stringi::stri_trans_general(Comments$text, &quot;latin-ascii&quot;) #converts text encoding, otherwise the tokenizer won&#39;t retain contractions Comments$text &lt;- str_remove_all(Comments$text, url_regex) 2.4.2 Tokenize #tokenize G1Comm &lt;- Comments %&gt;% unnest_tokens(word, text) # %&gt;% #ungroup() #G2Comm &lt;- Comments %&gt;% # #group_by(subreddit) %&gt;% # unnest_tokens(bigram, text, token = #&quot;ngrams&quot;, n = 2, n_min = 2) #%&gt;% #repeat above as bigram #ungroup() 2.4.2.1 1-grams #Create object for numbers, so that we can remove them from the data nums &lt;- G1Comm %&gt;% filter(str_detect(word, &quot;^[0-9]&quot;)) %&gt;% select(word) %&gt;% unique() #Source: https://richpauloo.github.io/2017-12-29-Using-tidytext-to-make-word-clouds/ Bots &lt;- filter(G1Comm, word == &quot;bot&quot;) data(stop_words) CommTkns &lt;- G1Comm %&gt;% anti_join(stop_words) %&gt;% #take out stop words (i.e. &#39;a&#39;, &#39;an&#39;, &#39;of&#39;, &#39;the&#39;) anti_join(nums, by = &quot;word&quot;) %&gt;% filter(user %notin% Bots$user) %&gt;% filter(!grepl(&quot;_&quot;, .$word)) #ungroup() G1Comm &lt;- CommTkns paste(&quot;Removed&quot;, nrow(CommData) - nrow(Comments),&quot;tokens&quot;) ## [1] &quot;Removed 12771 tokens&quot; 2.4.2.2 2-grams (Clean) 2-gram code is not used in analysis due to unsolved cleaning error. #G2Comm_separated &lt;- G2Comm %&gt;% #Separate bigrams to remove stopwords and transitional phrases # separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) #run boolean functions for cleaning on `word1` and `word2` #G2Comm_filtered &lt;- G2Comm_separated %&gt;% # filter(!word1 %in% stop_words$word) %&gt;% #remove stop words # filter(!word2 %in% stop_words$word) %&gt;% # filter(!word1 == &quot;NA&quot;) %&gt;% #remove anomalous strings # filter(!word2 == &quot;NA&quot;) %&gt;% # filter(!word1 %in% nums) %&gt;% #remove numbers # filter(!word2 %in% nums) #G2Comm &lt;- G2Comm_filtered %&gt;% # unite(bigram, word1, word2, sep = &quot; &quot;)#reunify cleaned word columns #G2Comm &lt;- G2Comm[!grepl(&quot;_&quot;, G2Comm$bigram),] #rm(G2Comm_separated) #rm(G2Comm_filtered) 2.4.3 Count G1CommCount &lt;- G1Comm %&gt;% group_by(subreddit) %&gt;% #group words by affiliation label count(subreddit, word, sort = FALSE) %&gt;% #count and create column &#39;n&#39; ungroup() #G2CommCount &lt;- G2Comm %&gt;% #repeat for bigrams # group_by(subreddit) %&gt;% # count(bigram, sort = FALSE) %&gt;% # ungroup() rm(G1Comm) #rm(G2Comm) 2.5 Export #write.csv(G1SubmCount, &quot;clean_data/1GSubm.csv&quot;, row.names = FALSE) #write.csv(G2SubmCount, &quot;clean_data/2GSubm.csv&quot;, row.names = FALSE) write.csv(SubmissionTkns, paste0(dataDir, &quot;/SubmTkns.csv&quot;), row.names = FALSE) write.csv(CommTkns, paste0(dataDir, &quot;/CommTkns.csv&quot;), row.names = FALSE) 2.6 Works Cited Boe B (2014). PRAW: The Python Reddit API Wrapper. 2012-, https://github.com/praw-dev/praw/ [Online; accessed 2017-09-29]. Silge J, Robinson D (2016). tidytext: Text Mining and Analysis Using Tidy Data Principles in R. JOSS, 1(3). doi: 10.21105/joss.00037, http://dx.doi.org/10.21105/joss.00037. dim(Comments) ## [1] 164800 8 "],["token-analysis.html", "Chapter 3 Token Analysis 3.1 Import 3.2 Token Counts 3.3 Wordclouds 3.4 TF-IDF 3.5 Posts over time 3.6 Currencies", " Chapter 3 Token Analysis [In-Progress] knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = TRUE) # set to TRUE to run this on only one reference file debugging &lt;- FALSE # this will expect the file or files to be in a subdirectory with the following name refsource &lt;- &quot;MungingProj2&quot; dataDir &lt;- &quot;Proj2Data&quot; workingDir &lt;- refsource # prefixes for all File reads and writes # titles for tables titletext &lt;- &quot;RedditCrypto&quot; srs = c(&quot;CryptoCurrency&quot;,&quot;CryptoMarkets&quot;) `%notin%` &lt;- Negate(`%in%`) 3.1 Import #Submissions #Subms1G&lt;- read.csv(paste0(dataDir, &quot;/clean_data/1GComms.csv&quot;)) #token counts by pol_group #Subms2G&lt;- read.csv(&quot;clean_data/2GSubm.csv&quot;) #token counts by #TknsS&lt;- read.csv(paste0(dataDir, &quot;/SubmTkns.csv&quot;)) #token counts (full dataset) #Comments #Comms1G&lt;- read.csv(paste0(dataDir, &quot;/clean_data/1GComms.csv&quot;)) #Comms2G &lt;- read.csv(&quot;clean_data/2GComms.csv&quot;) TknsC&lt;- read.csv(paste0(dataDir, &quot;/CommTkns.csv&quot;)) #Matrices #MtrxS &lt;- read.csv(&quot;clean_data/SubMtrx.csv&quot;) #MtrxC &lt;- read.csv(paste0(dataDir,&quot;/CommMtrx.csv&quot;)) #MtrxU &lt;- read.csv(paste0(dataDir, &quot;/UserMtrx.csv&quot;)) 3.2 Token Counts Token counts are done at several different levels. The first being word frequency within each subreddit, which allows for a comparison of common words across communitas. The second is word frequency within each subreddit page (i.e. Top, Controversial, Hot), which allows for a comparison of common words across strata within each communitas. allCount &lt;-TknsC %&gt;% group_by(subreddit) %&gt;% #group words by affiliation label count(word, sort = TRUE) %&gt;% ungroup() 3.2.1 Most Frequent Tokens The following code show the top 25 most frequently occuring words within each subreddit. sr_all_n25 &lt;- allCount %&gt;% group_by(subreddit) %&gt;% top_n(25) %&gt;% ungroup() 3.3 Wordclouds Wordclouds are then constructed using the top 50 words from each subreddit page. 3.3.1 All words (Submissions and Comments) #sr_allCount &lt;- allCount %&gt;% filter(!subreddit == &quot;r/all [control]&quot;) sr_allWC &lt;- allCount %&gt;% top_n(100) %&gt;% mutate(prop = n / max(n)) color=diverge_hcl(length(sr_allWC$prop))[rank(sr_allWC$prop)] set.seed(29) ggplot(sr_allWC, aes(label = word, size = prop, color = prop)) + geom_text_wordcloud_area(shape = &#39;circle&#39;, rm_outside = TRUE) + scale_size_area(max_size = 30) + #scale_colour_gradient2(low = &quot;black&quot;, mid = &quot;red4&quot;, high = &quot;red&quot;, space = &quot;Lab&quot;, aesthetics = &quot;color&quot;) + theme_minimal() ggsave(&quot;CC_wordcloud.pdf&quot;, device = &quot;pdf&quot;, path = &quot;CCViz&quot;, height = 10, width = 16) TF_IDF is a tool used to show distinct word usage across groups by taking the difference between the term frequency tf and inverse document frequency idf, ultimately revealing the tokens that are most unique to each subreddit. #Create TF-IDF matrix for Controversial Submissions allIDF &lt;- allCount %&gt;% bind_tf_idf(word, subreddit, n) %&gt;% #construct tf_idf by affiliation label arrange(desc(tf_idf)) %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) 3.4 TF-IDF The below figure shows that the religious communitas can be identified through the symbols presented in their distinguishing tokens. This methodology is useful for exploring the divergent symbolic identities of collective consciousnesses. The tf-ifd analysis below presents us with sets of names, concepts, and phrases that reference the symbolic systems of each religious identity. allIDF %&gt;% group_by(subreddit) %&gt;% top_n(10) %&gt;% ungroup()%&gt;% ggplot(aes(word, tf_idf, fill = subreddit)) + geom_col(show.legend = FALSE) + labs(x = NULL, y = &quot;tf-idf&quot;) + facet_wrap(~subreddit, ncol = 2, scales = &quot;free_y&quot;) + coord_flip() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) ggsave(&quot;tf_idf.pdf&quot;, device = &quot;pdf&quot;, path = &quot;CCViz&quot;, height = 8, width = 6) 3.5 Posts over time TknsByDate &lt;- TknsC %&gt;% separate(created, c(&quot;created&quot;, &quot;time&quot;), &quot; &quot;) %&gt;% mutate(created = ymd(created)) %&gt;% mutate_at(vars(created), funs(year, month, day)) TknsByMonth &lt;-TknsByDate %&gt;% filter(year &gt; 2015) %&gt;% mutate(Month = make_date(year, month)) monthCount &lt;- TknsByMonth %&gt;% group_by(subreddit, month, year) %&gt;% #group words by affiliation label count(word, sort = TRUE) %&gt;% #count and create column &#39;n&#39; top_n(10, n) %&gt;% ungroup() monthCount &lt;- monthCount %&gt;% group_by(subreddit, month, year) %&gt;% mutate(prop = n / max(n)) table(monthCount$subreddit) ## ## CryptoCurrency CryptoMarkets ## 396 435 ggplot(monthCount, aes( label = word, size = prop, color = prop )) + geom_text_wordcloud_area(rm_outside = TRUE) + scale_size_area(max_size = 20) + #scale_colour_gradient2(low = &quot;black&quot;, mid = &quot;red4&quot;, high = &quot;red&quot;, space = &quot;Lab&quot;, aesthetics = &quot;color&quot;) + theme_minimal() + facet_grid(vars(month), vars(year)) ggsave( &quot;CC_timeline.pdf&quot;, device = &quot;pdf&quot;, path = &quot;CCViz&quot;, height = 30, width = 20 ) 3.6 Currencies BTC &lt;- c(&quot;Bitcoin&quot;, &quot;bitcoin&quot;, &quot;BTC&quot;, &quot;btc&quot;) ETH &lt;- c(&quot;Ethereum&quot;, &quot;ethereum&quot;, &quot;ETH&quot;, &quot;eth&quot;) XRP &lt;- c(&quot;Ripple&quot;, &quot;ripple&quot;, &quot;XRP&quot;, &quot;xrp&quot;) LTC &lt;- c(&quot;Litecoin&quot;, &quot;litecoin&quot;, &quot;LTC&quot;, &quot;ltc&quot;) LINK &lt;- c(&quot;Chainlink&quot;, &quot;chainlink&quot;, &quot;LINK&quot;, &quot;link&quot;) currencies &lt;- c(BTC, ETH, XRP, LTC, LINK) # For frequency analysis CurTkns &lt;- TknsByDate %&gt;% filter(word %in% currencies) # For sentiment analysis CurComms &lt;- CommData %&gt;% filter(comm_id %in% CurTkns$comm_id) 3.6.1 Assigning Identifiers CurTkns &lt;- CurTkns %&gt;% mutate(Coin = case_when( .$word %in% BTC ~ &quot;BTC&quot;, .$word %in% ETH ~ &quot;ETH&quot;, .$word %in% XRP ~ &quot;XRP&quot;, .$word %in% LTC ~ &quot;LTC&quot;, .$word %in% LINK ~ &quot;LINK&quot; )) curTknsByMonth &lt;-CurTkns %&gt;% mutate(Month = make_date(year, month)) curCounts &lt;- curTknsByMonth %&gt;% group_by(Coin) %&gt;% #group words by affiliation label count(word, sort = TRUE) %&gt;% #count and create column &#39;n&#39; ungroup() curCountsbyMonth &lt;- curTknsByMonth %&gt;% group_by(Coin, Month) %&gt;% #group words by affiliation label count(word, sort = TRUE) %&gt;% #count and create column &#39;n&#39; ungroup() ggplot(curCounts) + geom_bar(aes(x = Coin, y = n, fill = Coin), stat = &quot;identity&quot;) ggplot(curCountsbyMonth) + geom_bar(aes(x = Coin, y = n, fill = Coin), stat = &quot;identity&quot;) + facet_wrap(~Month) "]]
