[["index.html", "An analysis of Cryptocurrency Preface", " An analysis of Cryptocurrency Kalani Stanton, Timothy McCormack, Wei Chun Tien, 2020-11-19 Preface This project is an exploration of language in the comments from the two most popular subreddits oriented towards the discussion of Crypto-currencies and the trading of block-chain assets. The methodologies used herein include tokenization, sentiment analysis using dictionaries, and associative visualization primarily using packages found within the tidyverse. "],["intro.html", "Section 1 Introduction", " Section 1 Introduction This project is an exploration of the relationship between sentiment and the evaluations. Reddit, a social-engagement platform designed as a democratic forum of ideas, wherein each user can submit content of their own and evaluate that of other users. These forums are located within subreddits; subsections of the website dissociated according some symbolic purpose. Crypto-currencies are an interesting area to study within this context because the block-chain technologies, upon which these digital assets are built, are democratic by there very nature. To speak precisely, the value of a crypto-currency coin is ultimately determined by the interdependent evaluations of its holders. In one of the early transactions made using the currency, 10000 BTC was worth two pizzas; today, a single Bitcoin is worth a used car. This is surprising considering that, until very recently, these crypto-currencies were widely dismissed by global financial institutions; but as time goes on, more governments, banks, and brokers are finding value in this intangible asset. Despite the hesitancy of major financial institutions in the early years of the technology, Bitcoin and other block-chain based currencies, often referred to as Alt-Coins, achieved confounding success in the later half of the 2010s. Since then it has fallen significantly, but today the price of Bitcoin is rapidly increasing and is on the verge of breaking its previous record set in 2017. Regarding all of this, it is once again an appropriate time to ask: Why? This project does not seek to answer this question, nor will it make make any empirical claims about the reliability or feasibility of using social media data to predict the value of currencies. Instead, this project is an exploratory analysis of data to assess the viability of using sentiment to predict the value of block-chain based currencies. library(tidyverse) library(tidytext) library(textclean) library(dplyr) library(stringr) library(knitr) library(wordcloud) library(kableExtra) library(DT) library(tidygraph) library(ggraph) library(tm) library(ggwordcloud) library(Hmisc) library(lubridate) library(wordcloud) library(viridis) library(colorspace) library(Quandl) ## Warning: package &#39;Quandl&#39; was built under R version 4.0.3 ## Warning: package &#39;xts&#39; was built under R version 4.0.3 library(reshape) ## Warning: package &#39;reshape&#39; was built under R version 4.0.3 library(hrbrthemes) ## Warning: package &#39;hrbrthemes&#39; was built under R version 4.0.3 library(scales) library(lubridate) library(plotly) "],["clean.html", "Section 2 Cleaning and Tokenization 2.1 Import Clean 2.2 Submissions 2.3 Comments 2.4 Export", " Section 2 Cleaning and Tokenization knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = TRUE) # set to TRUE to run this on only one reference file debugging &lt;- FALSE # this will expect the file or files to be in a subdirectory with the following name refsource &lt;- &quot;MungingProj2&quot; dataDir &lt;- &quot;Proj2Data&quot; workingDir &lt;- refsource # prefixes for all File reads and writes # titles for tables titletext &lt;- &quot;RedditCrypto&quot; srs = c(&quot;CryptoCurrency&quot;,&quot;CryptoMarkets&quot;) `%notin%` &lt;- Negate(`%in%`) 2.1 Import Clean The data imported in the following code chunk was collected using the Python Reddit API Wrapper to extract data from the subreddits in this study subm_fnames &lt;- list.files(dataDir, pattern = &quot;*_submissions.csv&quot;, full.names = TRUE) comm_fnames &lt;- list.files(dataDir, pattern = &quot;*_comments.csv&quot;, full.names = TRUE) subr_fnames &lt;- list.files(dataDir, pattern = &quot;*_subreddit.csv&quot;, full.names = TRUE) for (i in 1:length(subm_fnames)) assign(srs[i], read.csv(subm_fnames[i])) SubmData&lt;- rbind(CryptoCurrency, CryptoMarkets) for (i in 1:length(comm_fnames)) assign(srs[i], read.csv(comm_fnames[i])) CommData&lt;- rbind(CryptoCurrency, CryptoMarkets) for (i in 1:length(subr_fnames)) assign(srs[i], read.csv(subr_fnames[i])) SubrData&lt;- rbind(CryptoCurrency, CryptoMarkets) 2.1.1 Fix Names Subreddits are identifiable among online communities in that they are often referenced according to the url suffix of the subreddit webpage (i.e. r/CryptoCurrency comes from reddit.com/r/CryptoCurrency). To retain this cultural identifier, we modify the names srs &lt;- unique(SubmData$subreddit) SubmData &lt;- SubmData %&gt;% mutate( subreddit = case_when( .$subreddit == srs[1] ~ &quot;r/CryptoCurrency&quot;, .$subreddit == srs[2] ~ &quot;r/CryptoMarkets&quot;) ) %&gt;% ungroup() srs &lt;- unique(CommData$subreddit) CommData &lt;- CommData %&gt;% mutate( subreddit = case_when( .$subreddit == srs[1] ~ &quot;r/CryptoCurrency&quot;, .$subreddit == srs[2] ~ &quot;r/CryptoMarkets&quot;) ) %&gt;% ungroup() 2.1.2 Subreddit information submnums &lt;- table(SubmData$subreddit) SubmNums &lt;- as.data.frame(submnums, .name_repair = &quot;minimal&quot;) colnames(SubmNums)[1] &lt;- &quot;Subreddit&quot; ggplot(SubmNums, aes(x = Subreddit, y = Freq, fill = Subreddit)) + geom_bar(stat = &quot;identity&quot;) + scale_y_continuous(name=&quot;# of Submissions by Subreddit&quot;, labels = scales::comma) c &lt;- CommData commnums &lt;- table(c$subreddit) CommNums &lt;- as.data.frame(commnums, .name_repair = &quot;minimal&quot;) colnames(CommNums)[1] &lt;- &quot;Subreddit&quot; ggplot(CommNums, aes(x = Subreddit, y = Freq, fill = Subreddit)) + geom_bar(stat = &quot;identity&quot;) + scale_y_continuous(name=&quot;# of Comments by Subreddit (from 300 posts each)&quot;, labels = scales::comma) kable(SubrData) %&gt;% kable_styling(&quot;striped&quot;, full_width = F) X title subscribers created public_description 0 Cryptocurrency News &amp; Discussion 1117855 2013-03-11 17:51:50 The official source for CryptoCurrency News, Discussion &amp; Analysis. 0 r/CryptoMarkets 236835 2013-11-12 18:50:17 FOREX community for cryptocurrencies. Tags: mt gox bitcoin, long term potential, open source exchange, low inflation rate, demand and price, technical analysis, fundamentals, Bitcoin, Ethereum, Litecoin, Monero, Dash, Augur, token, volume, oscillator, RSI, stochastic, trend, sentiment, strategy, scam, coin, coinmarketcap, altcoin, Peercoin, script, blockchain, PoW, PoS, Proof of Work, Submissions from each subreddit: Comments in each subreddit: 2.2 Submissions The analysis of langauge begins by quantifying the presence of words in each Subm, through the process of tokenization. Tokens are discrete strings of words or characters that can be isolated as n-grams; with n pertaining to the number of words in each token. Tokens are pulled from the body of text that is most informative for the purposes of analysis. For submissions, the informative text is the title of the submission, which contains information on topics; whereas, for comments, the informative text is the comment itself. 2.2.1 Clean To ensure that our data accurately represent activity within the communitas, we want to ensure that each observation is a unique instance of engagement. Reposts tend to be common on reddit, so using a distinct() function on the text column will remove any duplicate posts. Additionally, any posts that are removed from the subreddits return an NA value in user column, thus we can remove deleted comments by filtering out all non user == NA. #Clean Encoding(SubmData$text) &lt;- &quot;UTF-8&quot; Submissions &lt;- SubmData %&gt;% group_by(user) %&gt;% filter(!is.na(user)) %&gt;% # Take out deleted comments ungroup() %&gt;% distinct(text, .keep_all = TRUE) #remove duplicate submissions paste(&quot;Removed&quot;, nrow(SubmData) - nrow(Submissions),&quot;submissions.&quot;) ## [1] &quot;Removed 14 submissions.&quot; data(stop_words) SubmissionTkns &lt;- Submissions %&gt;% group_by(subreddit) %&gt;% unnest_tokens(word, text) %&gt;% ungroup() As part of the tokenization process for submissions, we remove stop words (e.g. and, a, the) because we are interested in using our tokens to identify prevalent topics of discussion and attitudes in the subreddits. 2.2.1.1 Tokens #Create object for numbers, so that we can remove them from the data nums &lt;- SubmissionTkns %&gt;% filter(str_detect(word, &quot;^[0-9]&quot;)) %&gt;% select(word) %&gt;% unique() #Source: https://richpauloo.github.io/2017-12-29-Using-tidytext-to-make-word-clouds/ SubmissionTkns &lt;- SubmissionTkns %&gt;% anti_join(stop_words, by = &quot;word&quot;) %&gt;% anti_join(nums, by = &quot;word&quot;) %&gt;% filter(!grepl(&quot;_&quot;, .$word)) G1Subm &lt;- SubmissionTkns 2.3 Comments 2.3.1 Clean Encoding(CommData$text) &lt;- &quot;UTF-8&quot; Comments &lt;- CommData %&gt;% group_by(user) %&gt;% filter(!is.na(user)) %&gt;% # Take out deleted comments ungroup() %&gt;% distinct(text, .keep_all = TRUE) #remove duplicate comments paste(&quot;Removed&quot;, nrow(CommData) - nrow(Comments),&quot;comments&quot;) ## [1] &quot;Removed 12771 comments&quot; url_regex &lt;- &quot;http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+&quot; Comments$text &lt;-stringi::stri_trans_general(Comments$text, &quot;latin-ascii&quot;) #converts text encoding, otherwise the tokenizer won&#39;t retain contractions Comments$text &lt;- str_remove_all(Comments$text, url_regex) 2.3.1.1 Tokenize #tokenize G1Comm &lt;- Comments %&gt;% unnest_tokens(word, text) #Create object for numbers, so that we can remove them from the data nums &lt;- G1Comm %&gt;% filter(str_detect(word, &quot;^[0-9]&quot;)) %&gt;% select(word) %&gt;% unique() #Source: https://richpauloo.github.io/2017-12-29-Using-tidytext-to-make-word-clouds/ Bots &lt;- filter(G1Comm, word == &quot;bot&quot;) data(stop_words) CommTkns &lt;- G1Comm %&gt;% anti_join(stop_words) %&gt;% #take out stop words (i.e. &#39;a&#39;, &#39;an&#39;, &#39;of&#39;, &#39;the&#39;) anti_join(nums, by = &quot;word&quot;) %&gt;% filter(user %notin% Bots$user) %&gt;% filter(!grepl(&quot;_&quot;, .$word)) #ungroup() G1Comm &lt;- CommTkns paste(&quot;Removed&quot;, nrow(CommData) - nrow(Comments),&quot;tokens&quot;) ## [1] &quot;Removed 12771 tokens&quot; 2.4 Export write.csv(SubmissionTkns, paste0(dataDir, &quot;/SubmTkns.csv&quot;), row.names = FALSE) write.csv(CommTkns, paste0(dataDir, &quot;/CommTkns.csv&quot;), row.names = FALSE) "],["token-analysis.html", "Section 3 Token Analysis 3.1 Import 3.2 Token Counts 3.3 Wordclouds 3.4 Currencies", " Section 3 Token Analysis This chapter contains the code for wrangling the cleaned and tokenized data into data-frames containing word frequencies and visualizing the data using the raw counts derived from this process. knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = TRUE) # set to TRUE to run this on only one reference file debugging &lt;- FALSE # this will expect the file or files to be in a subdirectory with the following name refsource &lt;- &quot;MungingProj2&quot; dataDir &lt;- &quot;Proj2Data&quot; workingDir &lt;- refsource # prefixes for all File reads and writes # titles for tables titletext &lt;- &quot;RedditCrypto&quot; srs = c(&quot;CryptoCurrency&quot;,&quot;CryptoMarkets&quot;) `%notin%` &lt;- Negate(`%in%`) 3.1 Import 3.2 Token Counts allCount &lt;-TknsC %&gt;% #group words by affiliation label count(word, sort = TRUE) %&gt;% ungroup() 3.2.1 Most Frequent Tokens The following code show the top 25 most frequently occuring words within each subreddit. sr_all_n25 &lt;- allCount %&gt;% top_n(25) %&gt;% ungroup() kable(sr_all_n25) word n people 18605 crypto 17983 bitcoin 13204 money 12433 time 9221 btc 9095 market 8906 buy 7798 coins 6151 coin 5784 price 5536 lot 4204 lol 4189 sell 3983 exchange 3971 currency 3949 shit 3833 day 3802 blockchain 3537 eth 3412 pay 3364 fees 3296 world 3259 yeah 3240 real 3209 3.3 Wordclouds Wordcloud is constructed using the top 100 words. #sr_allCount &lt;- allCount %&gt;% filter(!subreddit == &quot;r/all [control]&quot;) sr_allWC &lt;- allCount %&gt;% top_n(100) %&gt;% mutate(prop = n / max(n)) set.seed(29) ggplot(sr_allWC, aes(label = word, size = prop, color = prop)) + geom_text_wordcloud_area(shape = &#39;circle&#39;, rm_outside = TRUE) + scale_size_area(max_size = 30) + theme_minimal() ggsave(&quot;CC_wordcloud.pdf&quot;, device = &quot;pdf&quot;, path = &quot;CCViz&quot;, height = 10, width = 16) 3.3.1 Wordclouds over time TknsByDate &lt;- TknsC %&gt;% separate(created, c(&quot;created&quot;, &quot;time&quot;), &quot; &quot;) %&gt;% mutate(created = ymd(created)) %&gt;% mutate_at(vars(created), funs(year, month, day)) TknsByMonth &lt;-TknsByDate %&gt;% filter(year &gt; 2015) %&gt;% mutate(Month = make_date(year, month)) monthCount &lt;- TknsByMonth %&gt;% group_by(subreddit, month, year) %&gt;% #group words by affiliation label count(word, sort = TRUE) %&gt;% #count and create column &#39;n&#39; top_n(10, n) %&gt;% ungroup() monthCount &lt;- monthCount %&gt;% group_by(subreddit, month, year) %&gt;% mutate(prop = n / max(n)) table(monthCount$subreddit) ## ## r/CryptoCurrency r/CryptoMarkets ## 396 435 ggplot(monthCount, aes( label = word, size = prop, color = prop )) + geom_text_wordcloud_area(rm_outside = TRUE) + scale_size_area(max_size = 5) + theme_minimal() + facet_grid(vars(month), vars(year)) 3.4 Currencies BTC &lt;- c(&quot;Bitcoin&quot;, &quot;bitcoin&quot;, &quot;BTC&quot;, &quot;btc&quot;, &quot;Btc&quot;) ETH &lt;- c(&quot;Ethereum&quot;, &quot;ethereum&quot;, &quot;ETH&quot;, &quot;eth&quot;, &quot;Eth&quot;) XRP &lt;- c(&quot;Ripple&quot;, &quot;ripple&quot;, &quot;XRP&quot;, &quot;xrp&quot;, &quot;Xrp&quot;) LTC &lt;- c(&quot;Litecoin&quot;, &quot;litecoin&quot;, &quot;LTC&quot;, &quot;ltc&quot;, &quot;Ltc&quot;) currencies &lt;- c(BTC, ETH, XRP, LTC) # For frequency analysis CurTkns &lt;- TknsByDate %&gt;% filter(word %in% currencies) # For sentiment analysis CurComms &lt;- CommData %&gt;% filter(comm_id %in% CurTkns$comm_id) 3.4.1 Assigning Identifiers CurTkns &lt;- CurTkns %&gt;% mutate(Coin = case_when( .$word %in% BTC ~ &quot;BTC&quot;, .$word %in% ETH ~ &quot;ETH&quot;, .$word %in% XRP ~ &quot;XRP&quot;, .$word %in% LTC ~ &quot;LTC&quot; )) # Establish date column for grouping curTknsByMonth &lt;-CurTkns %&gt;% mutate(Month = make_date(year, month)) curCounts &lt;- curTknsByMonth %&gt;% group_by(Coin) %&gt;% #group words by affiliation label count(word, sort = TRUE) %&gt;% #count and create column &#39;n&#39; ungroup() curCountsbyMonth &lt;- curTknsByMonth %&gt;% group_by(Coin, Month) %&gt;% #group words by affiliation label count(word, sort = TRUE) %&gt;% #count and create column &#39;n&#39; ungroup() ggplot(curCounts) + geom_bar(aes(x = Coin, y = n, fill = Coin), stat = &quot;identity&quot;) ggplot(curCountsbyMonth) + geom_bar(aes(x = Coin, y = n, fill = Coin), stat = &quot;identity&quot;) + facet_wrap(~Month) write.csv(curTknsByMonth, paste0(dataDir, &quot;/CoinTknsMonthly.csv&quot;), row.names = FALSE) "],["sentiments.html", "Section 4 Sentiment Analysis 4.1 Data Shaping 4.2 Price &amp; Sentiment over Time 4.3 User Sentiment 4.4 Descriptive Statistics 4.5 Most Abundant Sentiment Over Time 4.6 Plots of Top Sentiment Over Time", " Section 4 Sentiment Analysis dataDir &lt;- &quot;Proj2Data&quot; curTknsByMonth &lt;- read.csv(paste0(dataDir, &quot;/CoinTknsMonthly.csv&quot;)) CommTkns &lt;- read.csv(paste0(dataDir, &quot;/CommTkns.csv&quot;)) 4.1 Data Shaping To group comments and their sentiments by Coin we have to first assign this identifier to the tokens via their associated comm_id. currTkns &lt;- CommTkns %&gt;% inner_join(curTknsByMonth[,c(&quot;Coin&quot;,&quot;Month&quot;,&quot;comm_id&quot;)], by = &quot;comm_id&quot;) 4.1.1 get_sentiments() nrc&lt;- get_sentiments(&quot;nrc&quot;) word sentiment abacus trust abandon fear abandon negative abandon sadness abandoned anger abandoned fear 4.1.2 Sentiment Counts By Coin #ggplot(sntmntByMonth, aes(x = Month, y = n, color = sentiment)) + #geom_line() + #facet_wrap(~Coin, scale = &quot;free_y&quot;) sBmPlot &lt;- ggplot(sntmntByMonth, aes(x = as.Date(Month), y = n, group = sentiment, color = sentiment)) + geom_line() + scale_x_date(labels = date_format(&quot;%Y&quot;)) + facet_wrap(~Coin) + xlab(&quot;Month&quot;) + ylab(&quot;Number of Tokens (n)&quot;) ggplotly(sBmPlot) # Quandl limits free users to 50 calls a day, so these code chunks can be run for more recent data #pBTC &lt;- Quandl(&#39;BITFINEX/BTCUSD&#39;) #pETH &lt;- Quandl(&#39;BITFINEX/ETHUSD&#39;) #pLTC &lt;- Quandl(&#39;BITFINEX/LTCUSD&#39;) #pXRP &lt;- Quandl(&#39;BITFINEX/XRPUSD&#39;) #pBTC[&quot;Coin&quot;] &lt;- &quot;BTC&quot; #pETH[&quot;Coin&quot;] &lt;- &quot;ETH&quot; #pLTC[&quot;Coin&quot;] &lt;- &quot;LTC&quot; #pXRP[&quot;Coin&quot;] &lt;- &quot;XRP&quot; #PricesByCoin &lt;- rbind(pBTC, pETH, pLTC, pXRP) #To get around this call limit, we use a csv to hold the data PricesByCoin&lt;- read.csv(paste0(dataDir, &quot;/pricesByCoin.csv&quot;)) #write.csv(PricesByCoin,paste0(dataDir,&quot;/pricesByCoin.csv&quot;)) 4.2 Price &amp; Sentiment over Time # function for getting Coin Price/ Sentiment vs Time graph by coin get_graph &lt;- function(coin, coeff) { # coin = &quot;COIN_NAME&quot;, coeff = Value used to transform sentiment to match price scale on graph # get related coin data coinprice_data &lt;- PricesByCoin %&gt;% filter(Coin == coin) coin_sntmntByMonth &lt;- sntmntByMonth %&gt;% filter(Coin == coin) # reshape coin price by day data to merge high, low, last, med into one variable price_by_mkt_metric &lt;- melt(coinprice_data, id = c(&quot;Date&quot;, &quot;Coin&quot;)) colnames(price_by_mkt_metric)[3] &lt;- &quot;Mkt_Metrics&quot; # normalize x-values for both datasets (date) price_by_mkt_metric$Date &lt;- as_date(price_by_mkt_metric$Date) coin_sntmntByMonth$Month &lt;- as_date(coin_sntmntByMonth$Month) # make the gg plot Coin_Daily_Price.plot &lt;- price_by_mkt_metric %&gt;% filter((Mkt_Metrics %in% c(&quot;High&quot;, &quot;Low&quot;, &quot;Last&quot;))) %&gt;% # ggplot setup ggplot(aes(x = Date)) + theme_minimal() + ggtitle(paste(coin, &quot; Sentiment/ Price vs Time&quot;)) + xlab(&quot;Date&quot;) + theme(legend.title = element_blank()) + # plot price vs time lines geom_line( stat = &#39;identity&#39;, aes( y = value, linetype = Mkt_Metrics, color = Mkt_Metrics, size = Mkt_Metrics, alpha = Mkt_Metrics)) + scale_linetype_manual(&quot;Market Metrics&quot;, values = c(&quot;solid&quot;, &quot;solid&quot;, &quot;solid&quot;)) + scale_color_manual(&quot;Market Metrics&quot;, values = c(&#39;#EF9A9A&#39;, &#39;#C5E1A5&#39;, &#39;#212121&#39;)) + scale_size_manual(&quot;Market Metrics&quot;, values = c(1, 1, 0.3)) + scale_alpha_manual(&quot;Market Metrics&quot;, values = c(0.8, 0.8, 1)) + # plot sentiment bars (stacked) geom_bar( data = coin_sntmntByMonth, stat = &#39;identity&#39;, aes( x = Month, y = n / coeff, fill = sentiment)) + # setup y-axises scale_y_continuous(name = &quot;Price (USD)&quot;, sec.axis = sec_axis( ~ . * coeff, name = &quot;Sentiment (n)&quot;)) # convert to plotly Coin_Daily_Price.plotly = ggplotly(Coin_Daily_Price.plot, tooltip = c(&quot;label&quot;,&quot;x&quot;,&quot;y&quot;)) # cleans up ledgend labels for (i in 1:length(Coin_Daily_Price.plotly$x$data)) { if (!is.null(Coin_Daily_Price.plotly$x$data[[i]]$name)) { Coin_Daily_Price.plotly$x$data[[i]]$name = gsub(&quot;\\\\(&quot;, &quot;&quot;, str_split(Coin_Daily_Price.plotly$x$data[[i]]$name, &quot;,&quot;)[[1]][1]) } } Coin_Daily_Price.plot Coin_Daily_Price.plotly # FOR THE LIFE OF ME CANNOT FIGURE OUT HOW TO GET THE 2ND AXIS TO SHOW } 4.2.1 BTC get_graph(&quot;BTC&quot;, 4) 4.2.2 ETH get_graph(&quot;ETH&quot;,20) 4.2.3 LTC get_graph(&quot;LTC&quot;, 19) 4.2.4 XRP get_graph(&quot;XRP&quot;, 8000) 4.3 User Sentiment posts_by_coin &lt;- currSntmntTkns %&gt;% group_by(Coin) %&gt;% count(Coin) users_by_coin &lt;- currSntmntTkns %&gt;% group_by(user) %&gt;% count(user) emo_stats &lt;- function(emote, color1, color2){ users.emo &lt;- currSntmntTkns %&gt;% filter(sentiment == emote) %&gt;% count(user) users.emo &lt;- inner_join(users.emo, users_by_coin, by = &quot;user&quot;, suffix = c(&quot;.emote&quot;, &quot;.total&quot;)) %&gt;% filter(n.total &gt;= 500) %&gt;% mutate(n.emote_porp = n.emote/n.total) %&gt;% arrange(desc(n.emote_porp) ) coin.emo &lt;- currSntmntTkns %&gt;% filter(sentiment == emote) %&gt;% group_by(Coin) %&gt;% count(Coin) coin.emo &lt;- inner_join(coin.emo, posts_by_coin, by = &quot;Coin&quot;, suffix = c(&quot;.emote&quot;, &quot;.total&quot;)) %&gt;% mutate(n.emote_porp=n.emote/n.total) #users_coin.emo TOTAL plot plt1 &lt;- ggplotly( ggplot(coin.emo, aes(x = Coin)) + theme_minimal() + theme(panel.grid.major.x = element_blank()) + geom_bar(aes(y = n.emote), stat=&#39;identity&#39;, fill = color1, width = 0.5) + ggtitle(&quot;Total emote by coin&quot;) + xlab(&quot;Coin&quot;) + ylab(&quot;Emote tokens (posts.emote)&quot;) ) # users_coin.emo PROP plot plt2 &lt;- ggplotly( ggplot(coin.emo, aes(x = Coin)) + theme_minimal() + theme(panel.grid.major.x = element_blank()) + geom_bar(aes(y = n.emote_porp), stat=&#39;identity&#39;, fill = color2, width = 0.5) + ggtitle(&quot;Proportion of emote by coin&quot;) + xlab(&quot;Coin&quot;) + ylab(&quot;Emote tokens (posts.emote/ posts.total)&quot;) ) return(list(as_tibble(users.emo), plt1, plt2)) } 4.3.1 Angriest Angriest users ang &lt;- emo_stats(&quot;anger&quot;, &quot;#F1948A&quot;, &quot;#E6B0AA&quot;) head(ang[[1]]) ## # A tibble: 6 x 4 ## user n.emote n.total n.emote_porp ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 adamlh 98 540 0.181 ## 2 tippr 392 2580 0.152 ## 3 HughHonee 99 675 0.147 ## 4 Cryptopricedrops 158 1120 0.141 ## 5 DontMicrowaveCats 90 650 0.138 ## 6 ggekko999 221 1599 0.138 ang[[2]] ang[[3]] 4.3.2 Happiest Top most joyful/ positive users hap &lt;- emo_stats(c(&quot;joy&quot;, &quot;positive&quot;), &quot;#F7DC6F&quot;, &quot;#FCF3CF&quot;) head(hap[[1]]) ## # A tibble: 6 x 4 ## user n.emote n.total n.emote_porp ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 NotGonnaGetBanned 962 2886 0.333 ## 2 Vincents_keyboard 300 1107 0.271 ## 3 Tribal_Tech 397 1466 0.271 ## 4 JohndeBoer 1187 4472 0.265 ## 5 cryptolicious501 163 674 0.242 ## 6 Hanzburger 205 851 0.241 hap[[2]] hap[[3]] 4.3.3 Saddest Top saddest/ most negative users sad &lt;- emo_stats(c(&quot;negative&quot;, &quot;sad&quot;), &quot;#5499C7&quot;, &quot;#D4E6F1&quot;) head(sad[[1]]) ## # A tibble: 6 x 4 ## user n.emote n.total n.emote_porp ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 japsock 96 556 0.173 ## 2 brobits 117 875 0.134 ## 3 ebringer 112 880 0.127 ## 4 DontMicrowaveCats 80 650 0.123 ## 5 Cryptopricedrops 136 1120 0.121 ## 6 Hypocriciety 110 919 0.120 sad[[2]] sad[[3]] 4.4 Descriptive Statistics ggplot(sntmntByMonth, aes(x = Coin, fill = Coin)) + geom_bar() + ggtitle(&quot;Distribution of Coin Types&quot;) ggplot(sntmntByMonth, aes(x = sentiment, fill = sentiment)) + geom_bar() + ggtitle(&quot;Distribution Sentiments Among All Coins&quot;) ggplot(sntmntByMonth, aes(x = Month, fill = Month)) + geom_bar() + ggtitle(&quot;Number of Sentiment Types for Each Month&quot;) ggplot(sntmntByMonth, aes(x = sentiment, y = n, color = Coin)) + geom_point() + ggtitle(&quot;Popularity of Each Sentiment Among Coin Type&quot;) 4.5 Most Abundant Sentiment Over Time aggSent &lt;- function(pop.df, n, m, coindf, unevenStep = FALSE){ k = 1 j = 1 for (i in 1:n){ if(coindf[i,2] == m[j]){ if(coindf[i,4] &gt; pop.df[j,3]){ pop.df[j,3] = coindf[i,4] pop.df[j,2] = coindf[i,3] pop.df[j,4] = pop.df[j,4] + coindf[i,4] pop.df[j,5] = pop.df[j,3] / pop.df[j,4] } } k = k + 1 if(k == 11){ j = j + 1 k = 1 } } return(pop.df) } df &lt;- sntmntByMonth df &lt;- na.omit(df) # create a new dataframe for each coin btc &lt;- df[which(df$Coin == &quot;BTC&quot;),] btc_m &lt;- unique(btc$Month) btc_n &lt;- nrow(btc) n &lt;- length(btc_m) sentiment &lt;- rep(&quot;x&quot;, n) btc.pop.df &lt;- data.frame(btc_m, sentiment,0, 0, 0) btc.pop.df &lt;- aggSent(btc.pop.df, btc_n, btc_m, btc) # ETH eth &lt;- df[which(df$Coin == &quot;ETH&quot;),] eth_m &lt;- unique(eth$Month) eth_n &lt;- nrow(eth) n &lt;- length(eth_m) sentiment &lt;- rep(&quot;x&quot;, n) eth.pop.df &lt;- data.frame(eth_m, sentiment,0, 0, 0) #eth.pop.df &lt;- aggSent(eth.pop.df, eth_n, eth_m, eth, unevenStep = TRUE) #eth.pop.df j &lt;- 1 for(i in 1:eth_n){ if(i &gt; 1){ prevM &lt;- eth[i-1, 2] month &lt;- eth[i,2] if(month != prevM){ j &lt;- j + 1 } } if(eth[i,2] == eth_m[j]){ if(eth[i,4] &gt; eth.pop.df[j,3]){ eth.pop.df[j,3] = eth[i,4] eth.pop.df[j,2] = eth[i,3] eth.pop.df[j,4] = eth.pop.df[j,4] + eth[i,4] eth.pop.df[j,5] = eth.pop.df[j,3] / eth.pop.df[j,4] } } } #xrp &lt;- df[which(df$Coin == &quot;XRP&quot;),] #eth.pop.df ltc &lt;- df[which(df$Coin == &quot;LTC&quot;),] ltc_m &lt;- unique(ltc$Month) ltc_n &lt;- nrow(ltc) n &lt;- length(ltc_m) sentiment &lt;- rep(&quot;x&quot;, n) ltc.pop.df &lt;- data.frame(ltc_m, sentiment,0, 0, 0) j &lt;- 1 for(i in 1:ltc_n){ if(i &gt; 1){ prevM &lt;- ltc[i-1, 2] month &lt;- ltc[i,2] if(month != prevM){ j &lt;- j + 1 } } if(ltc[i,2] == ltc_m[j]){ if(ltc[i,4] &gt; ltc.pop.df[j,3]){ ltc.pop.df[j,3] = ltc[i,4] ltc.pop.df[j,2] = ltc[i,3] ltc.pop.df[j,4] = ltc.pop.df[j,4] + ltc[i,4] ltc.pop.df[j,5] = ltc.pop.df[j,3] / ltc.pop.df[j,4] } } } ltc.pop.df ## ltc_m sentiment X0 X0.1 X0.2 ## 1 2017-08-01 positive 26 52 0.5000000 ## 2 2017-09-01 positive 67 116 0.5775862 ## 3 2017-10-01 positive 54 97 0.5567010 ## 4 2017-11-01 positive 229 442 0.5180995 ## 5 2017-12-01 positive 1077 2800 0.3846429 ## 6 2018-01-01 positive 886 2164 0.4094270 ## 7 2018-02-01 positive 531 1239 0.4285714 ## 8 2018-03-01 positive 161 466 0.3454936 ## 9 2018-04-01 positive 263 715 0.3678322 ## 10 2018-05-01 positive 134 260 0.5153846 ## 11 2018-06-01 positive 56 98 0.5714286 ## 12 2018-07-01 positive 31 56 0.5535714 ## 13 2018-08-01 positive 38 98 0.3877551 ## 14 2018-10-01 positive 21 34 0.6176471 ## 15 2018-11-01 positive 8 20 0.4000000 ## 16 2018-12-01 negative 11 18 0.6111111 ## 17 2019-01-01 positive 14 31 0.4516129 ## 18 2019-02-01 positive 18 46 0.3913043 ## 19 2019-04-01 positive 7 12 0.5833333 ## 20 2019-05-01 positive 18 55 0.3272727 ## 21 2019-06-01 positive 216 381 0.5669291 ## 22 2019-07-01 positive 14 35 0.4000000 ## 23 2019-09-01 positive 8 15 0.5333333 ## 24 2019-11-01 fear 2 3 0.6666667 ## 25 2020-02-01 negative 4 6 0.6666667 ## 26 2020-03-01 positive 78 204 0.3823529 ## 27 2020-05-01 anticipation 5 6 0.8333333 ## 28 2020-06-01 positive 127 229 0.5545852 ## 29 2020-07-01 positive 14 23 0.6086957 ## 30 2020-08-01 positive 22 45 0.4888889 ## 31 2020-09-01 positive 34 82 0.4146341 ## 32 2020-10-01 positive 43 77 0.5584416 xrp &lt;- df[which(df$Coin == &quot;XRP&quot;),] xrp_m &lt;- unique(xrp$Month) xrp_n &lt;- nrow(xrp) n &lt;- length(xrp_m) sentiment &lt;- rep(&quot;x&quot;, n) xrp.pop.df &lt;- data.frame(xrp_m, sentiment,0, 0, 0) j &lt;- 1 for(i in 1:xrp_n){ if(i &gt; 1){ prevM &lt;- xrp[i-1, 2] month &lt;- xrp[i,2] if(month != prevM){ j &lt;- j + 1 } } if(xrp[i,2] == xrp_m[j]){ if(xrp[i,4] &gt; xrp.pop.df[j,3]){ xrp.pop.df[j,3] = xrp[i,4] xrp.pop.df[j,2] = xrp[i,3] xrp.pop.df[j,4] = xrp.pop.df[j,4] + xrp[i,4] xrp.pop.df[j,5] = xrp.pop.df[j,3] / xrp.pop.df[j,4] } } } xrp.pop.df ## xrp_m sentiment X0 X0.1 X0.2 ## 1 2017-09-01 positive 42 110 0.3818182 ## 2 2017-10-01 positive 27 43 0.6279070 ## 3 2017-11-01 positive 53 113 0.4690265 ## 4 2017-12-01 positive 2784 6425 0.4333074 ## 5 2018-01-01 positive 2969 7678 0.3866892 ## 6 2018-02-01 positive 802 1483 0.5407957 ## 7 2018-03-01 positive 858 1943 0.4415852 ## 8 2018-04-01 positive 165 363 0.4545455 ## 9 2018-05-01 positive 571 1346 0.4242199 ## 10 2018-06-01 positive 339 822 0.4124088 ## 11 2018-07-01 positive 35 64 0.5468750 ## 12 2018-08-01 positive 16 33 0.4848485 ## 13 2018-09-01 anger 2 2 1.0000000 ## 14 2018-10-01 positive 18 41 0.4390244 ## 15 2018-11-01 positive 114 266 0.4285714 ## 16 2018-12-01 positive 199 430 0.4627907 ## 17 2019-01-01 negative 11 18 0.6111111 ## 18 2019-02-01 positive 39 141 0.2765957 ## 19 2019-03-01 negative 27 43 0.6279070 ## 20 2019-04-01 positive 17 30 0.5666667 ## 21 2019-05-01 positive 71 144 0.4930556 ## 22 2019-06-01 positive 285 707 0.4031117 ## 23 2019-07-01 positive 12 21 0.5714286 ## 24 2019-08-01 positive 10 22 0.4545455 ## 25 2019-09-01 positive 28 67 0.4179104 ## 26 2019-10-01 positive 33 74 0.4459459 ## 27 2019-12-01 positive 191 428 0.4462617 ## 28 2020-02-01 negative 12 25 0.4800000 ## 29 2020-03-01 positive 63 179 0.3519553 ## 30 2020-05-01 positive 232 389 0.5964010 ## 31 2020-06-01 positive 33 86 0.3837209 ## 32 2020-07-01 positive 24 77 0.3116883 ## 33 2020-08-01 negative 22 43 0.5116279 ## 34 2020-09-01 positive 10 17 0.5882353 ## 35 2020-10-01 negative 26 45 0.5777778 4.6 Plots of Top Sentiment Over Time ggplot(data = btc.pop.df, aes(x=btc_m, y = X0.2, group = 1, color = sentiment))+ geom_line()+ geom_point() + theme(axis.text.x = element_text(angle = 60, hjust = 1)) + ggtitle(&quot;Most Common Sentiment Over Time For BTC&quot;) + xlab(&quot;Month&quot;) + ylab(&quot;Proportion of Sentiment&quot;) ggplot(data = eth.pop.df, aes(x=eth_m, y = X0.2, group = 1, color = sentiment))+ geom_line()+ geom_point() + theme(axis.text.x = element_text(angle = 60, hjust = 1)) + ggtitle(&quot;Most Common Sentiment Over Time For ETH&quot;) + xlab(&quot;Month&quot;) + ylab(&quot;Proportion of Sentiment&quot;) ggplot(data = ltc.pop.df, aes(x=ltc_m, y = X0.2, group = 1, color = sentiment))+ geom_line()+ geom_point() + theme(axis.text.x = element_text(angle = 60, hjust = 1)) + ggtitle(&quot;Most Common Sentiment Over Time For LTC&quot;) + xlab(&quot;Month&quot;) + ylab(&quot;Proportion of Sentiment&quot;) ggplot(data = xrp.pop.df, aes(x=xrp_m, y = X0.2, group = 1, color = sentiment))+ geom_line()+ geom_point() + theme(axis.text.x = element_text(angle = 60, hjust = 1)) + ggtitle(&quot;Most Common Sentiment Over Time For XRP&quot;) + xlab(&quot;Month&quot;) + ylab(&quot;Proportion of Sentiment&quot;) "],["references.html", "References 4.7 Packages Used", " References #install.packages(&quot;devtools&quot;) #devtools::install_github(&quot;neuropsychology/report&quot;) library(report) #Package for citing 4.7 Packages Used # automatically create a bib database for R packages knitr::write_bib(c( .packages(), &#39;bookdown&#39;, &#39;knitr&#39;, &#39;rmarkdown&#39;,&#39;packages.bib&#39;, &quot;book.bib&quot;)) ## @Manual{R-base, ## title = {R: A Language and Environment for Statistical Computing}, ## author = {{R Core Team}}, ## organization = {R Foundation for Statistical Computing}, ## address = {Vienna, Austria}, ## year = {2020}, ## url = {https://www.R-project.org/}, ## } ## ## @Manual{R-bookdown, ## title = {bookdown: Authoring Books and Technical Documents with R Markdown}, ## author = {Yihui Xie}, ## year = {2020}, ## note = {R package version 0.21}, ## url = {https://CRAN.R-project.org/package=bookdown}, ## } ## ## @Manual{R-colorspace, ## title = {colorspace: A Toolbox for Manipulating and Assessing Colors and Palettes}, ## author = {Ross Ihaka and Paul Murrell and Kurt Hornik and Jason C. Fisher and Reto Stauffer and Claus O. Wilke and Claire D. McWhite and Achim Zeileis}, ## year = {2019}, ## note = {R package version 1.4-1}, ## url = {https://CRAN.R-project.org/package=colorspace}, ## } ## ## @Manual{R-dplyr, ## title = {dplyr: A Grammar of Data Manipulation}, ## author = {Hadley Wickham and Romain François and Lionel { ## Henry} and Kirill Müller}, ## year = {2020}, ## note = {R package version 1.0.2}, ## url = {https://CRAN.R-project.org/package=dplyr}, ## } ## ## @Manual{R-DT, ## title = {DT: A Wrapper of the JavaScript Library &#39;DataTables&#39;}, ## author = {Yihui Xie and Joe Cheng and Xianying Tan}, ## year = {2020}, ## note = {R package version 0.15}, ## url = {https://CRAN.R-project.org/package=DT}, ## } ## ## @Manual{R-forcats, ## title = {forcats: Tools for Working with Categorical Variables (Factors)}, ## author = {Hadley Wickham}, ## year = {2020}, ## note = {R package version 0.5.0}, ## url = {https://CRAN.R-project.org/package=forcats}, ## } ## ## @Manual{R-Formula, ## title = {Formula: Extended Model Formulas}, ## author = {Achim Zeileis and Yves Croissant}, ## year = {2018}, ## note = {R package version 1.2-3}, ## url = {https://CRAN.R-project.org/package=Formula}, ## } ## ## @Manual{R-ggplot2, ## title = {ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics}, ## author = {Hadley Wickham and Winston Chang and Lionel Henry and Thomas Lin Pedersen and Kohske Takahashi and Claus Wilke and Kara Woo and Hiroaki Yutani and Dewey Dunnington}, ## year = {2020}, ## note = {R package version 3.3.2}, ## url = {https://CRAN.R-project.org/package=ggplot2}, ## } ## ## @Manual{R-ggraph, ## title = {ggraph: An Implementation of Grammar of Graphics for Graphs and Networks}, ## author = {Thomas Lin Pedersen}, ## year = {2020}, ## note = {R package version 2.0.3}, ## url = {https://CRAN.R-project.org/package=ggraph}, ## } ## ## @Manual{R-ggwordcloud, ## title = {ggwordcloud: A Word Cloud Geom for &#39;ggplot2&#39;}, ## author = {Erwan {Le Pennec} and Kamil Slowikowski}, ## year = {2019}, ## note = {R package version 0.5.0}, ## url = {https://CRAN.R-project.org/package=ggwordcloud}, ## } ## ## @Manual{R-Hmisc, ## title = {Hmisc: Harrell Miscellaneous}, ## author = {Frank E {Harrell Jr} and with contributions from Charles Dupont and many others.}, ## year = {2020}, ## note = {R package version 4.4-1}, ## url = {https://CRAN.R-project.org/package=Hmisc}, ## } ## ## @Manual{R-hrbrthemes, ## title = {hrbrthemes: Additional Themes, Theme Components and Utilities for &#39;ggplot2&#39;}, ## author = {Bob Rudis}, ## year = {2020}, ## note = {R package version 0.8.0}, ## url = {https://CRAN.R-project.org/package=hrbrthemes}, ## } ## ## @Manual{R-kableExtra, ## title = {kableExtra: Construct Complex Table with &#39;kable&#39; and Pipe Syntax}, ## author = {Hao Zhu}, ## year = {2020}, ## note = {R package version 1.2.1}, ## url = {https://CRAN.R-project.org/package=kableExtra}, ## } ## ## @Manual{R-knitr, ## title = {knitr: A General-Purpose Package for Dynamic Report Generation in R}, ## author = {Yihui Xie}, ## year = {2020}, ## note = {R package version 1.29}, ## url = {https://CRAN.R-project.org/package=knitr}, ## } ## ## @Manual{R-lattice, ## title = {lattice: Trellis Graphics for R}, ## author = {Deepayan Sarkar}, ## year = {2020}, ## note = {R package version 0.20-41}, ## url = {https://CRAN.R-project.org/package=lattice}, ## } ## ## @Manual{R-lubridate, ## title = {lubridate: Make Dealing with Dates a Little Easier}, ## author = {Vitalie Spinu and Garrett Grolemund and Hadley Wickham}, ## year = {2020}, ## note = {R package version 1.7.9}, ## url = {https://CRAN.R-project.org/package=lubridate}, ## } ## ## @Manual{R-NLP, ## title = {NLP: Natural Language Processing Infrastructure}, ## author = {Kurt Hornik}, ## year = {2018}, ## note = {R package version 0.2-0}, ## url = {https://CRAN.R-project.org/package=NLP}, ## } ## ## @Manual{R-plotly, ## title = {plotly: Create Interactive Web Graphics via &#39;plotly.js&#39;}, ## author = {Carson Sievert and Chris Parmer and Toby Hocking and Scott Chamberlain and Karthik Ram and Marianne Corvellec and Pedro Despouy}, ## year = {2020}, ## note = {R package version 4.9.2.1}, ## url = {https://CRAN.R-project.org/package=plotly}, ## } ## ## @Manual{R-purrr, ## title = {purrr: Functional Programming Tools}, ## author = {Lionel Henry and Hadley Wickham}, ## year = {2020}, ## note = {R package version 0.3.4}, ## url = {https://CRAN.R-project.org/package=purrr}, ## } ## ## @Manual{R-Quandl, ## title = {Quandl: API Wrapper for Quandl.com}, ## author = {{Raymond McTaggart} and {Gergely Daroczi} and {Clement Leung}}, ## year = {2019}, ## note = {R package version 2.10.0}, ## url = {https://CRAN.R-project.org/package=Quandl}, ## } ## ## @Manual{R-RColorBrewer, ## title = {RColorBrewer: ColorBrewer Palettes}, ## author = {Erich Neuwirth}, ## year = {2014}, ## note = {R package version 1.1-2}, ## url = {https://CRAN.R-project.org/package=RColorBrewer}, ## } ## ## @Manual{R-readr, ## title = {readr: Read Rectangular Text Data}, ## author = {Hadley Wickham and Jim Hester and Romain Francois}, ## year = {2018}, ## note = {R package version 1.3.1}, ## url = {https://CRAN.R-project.org/package=readr}, ## } ## ## @Manual{R-report, ## title = {report: Automated Reporting of Statistical Models in R}, ## author = {Dominique Makowski and Daniel Lüdecke and Mattan S. Ben-Shachar}, ## year = {2020}, ## note = {R package version 0.1.0}, ## url = {https://easystats.github.io/report/}, ## } ## ## @Manual{R-reshape, ## title = {reshape: Flexibly Reshape Data}, ## author = {Hadley Wickham}, ## year = {2018}, ## note = {R package version 0.8.8}, ## url = {https://CRAN.R-project.org/package=reshape}, ## } ## ## @Manual{R-rmarkdown, ## title = {rmarkdown: Dynamic Documents for R}, ## author = {JJ Allaire and Yihui Xie and Jonathan McPherson and Javier Luraschi and Kevin Ushey and Aron Atkins and Hadley Wickham and Joe Cheng and Winston Chang and Richard Iannone}, ## year = {2020}, ## note = {R package version 2.4}, ## url = {https://CRAN.R-project.org/package=rmarkdown}, ## } ## ## @Manual{R-scales, ## title = {scales: Scale Functions for Visualization}, ## author = {Hadley Wickham and Dana Seidel}, ## year = {2020}, ## note = {R package version 1.1.1}, ## url = {https://CRAN.R-project.org/package=scales}, ## } ## ## @Manual{R-stringr, ## title = {stringr: Simple, Consistent Wrappers for Common String Operations}, ## author = {Hadley Wickham}, ## year = {2019}, ## note = {R package version 1.4.0}, ## url = {https://CRAN.R-project.org/package=stringr}, ## } ## ## @Manual{R-survival, ## title = {survival: Survival Analysis}, ## author = {Terry M Therneau}, ## year = {2020}, ## note = {R package version 3.1-12}, ## url = {https://CRAN.R-project.org/package=survival}, ## } ## ## @Manual{R-textclean, ## title = {textclean: Text Cleaning Tools}, ## author = {Tyler Rinker}, ## year = {2018}, ## note = {R package version 0.9.3}, ## url = {https://CRAN.R-project.org/package=textclean}, ## } ## ## @Manual{R-tibble, ## title = {tibble: Simple Data Frames}, ## author = {Kirill Müller and Hadley Wickham}, ## year = {2020}, ## note = {R package version 3.0.3}, ## url = {https://CRAN.R-project.org/package=tibble}, ## } ## ## @Manual{R-tidygraph, ## title = {tidygraph: A Tidy API for Graph Manipulation}, ## author = {Thomas Lin Pedersen}, ## year = {2020}, ## note = {R package version 1.2.0}, ## url = {https://CRAN.R-project.org/package=tidygraph}, ## } ## ## @Manual{R-tidyr, ## title = {tidyr: Tidy Messy Data}, ## author = {Hadley Wickham}, ## year = {2020}, ## note = {R package version 1.1.2}, ## url = {https://CRAN.R-project.org/package=tidyr}, ## } ## ## @Manual{R-tidytext, ## title = {tidytext: Text Mining using &#39;dplyr&#39;, &#39;ggplot2&#39;, and Other Tidy Tools}, ## author = {David Robinson and Julia Silge}, ## year = {2020}, ## note = {R package version 0.2.5}, ## url = {https://CRAN.R-project.org/package=tidytext}, ## } ## ## @Manual{R-tidyverse, ## title = {tidyverse: Easily Install and Load the &#39;Tidyverse&#39;}, ## author = {Hadley Wickham}, ## year = {2019}, ## note = {R package version 1.3.0}, ## url = {https://CRAN.R-project.org/package=tidyverse}, ## } ## ## @Manual{R-tm, ## title = {tm: Text Mining Package}, ## author = {Ingo Feinerer and Kurt Hornik}, ## year = {2019}, ## note = {R package version 0.7-7}, ## url = {https://CRAN.R-project.org/package=tm}, ## } ## ## @Manual{R-viridis, ## title = {viridis: Default Color Maps from &#39;matplotlib&#39;}, ## author = {Simon Garnier}, ## year = {2018}, ## note = {R package version 0.5.1}, ## url = {https://CRAN.R-project.org/package=viridis}, ## } ## ## @Manual{R-viridisLite, ## title = {viridisLite: Default Color Maps from &#39;matplotlib&#39; (Lite Version)}, ## author = {Simon Garnier}, ## year = {2018}, ## note = {R package version 0.3.0}, ## url = {https://CRAN.R-project.org/package=viridisLite}, ## } ## ## @Manual{R-wordcloud, ## title = {wordcloud: Word Clouds}, ## author = {Ian Fellows}, ## year = {2018}, ## note = {R package version 2.6}, ## url = {https://CRAN.R-project.org/package=wordcloud}, ## } ## ## @Manual{R-xts, ## title = {xts: eXtensible Time Series}, ## author = {Jeffrey A. Ryan and Joshua M. Ulrich}, ## year = {2020}, ## note = {R package version 0.12.1}, ## url = {https://CRAN.R-project.org/package=xts}, ## } ## ## @Manual{R-zoo, ## title = {zoo: S3 Infrastructure for Regular and Irregular Time Series (Z&#39;s ## Ordered Observations)}, ## author = {Achim Zeileis and Gabor Grothendieck and Jeffrey A. Ryan}, ## year = {2020}, ## note = {R package version 1.8-8}, ## url = {https://CRAN.R-project.org/package=zoo}, ## } ## ## @Book{bookdown2016, ## title = {bookdown: Authoring Books and Technical Documents with {R} Markdown}, ## author = {Yihui Xie}, ## publisher = {Chapman and Hall/CRC}, ## address = {Boca Raton, Florida}, ## year = {2016}, ## note = {ISBN 978-1138700109}, ## url = {https://github.com/rstudio/bookdown}, ## } ## ## @TechReport{colorspace2019a, ## title = {{colorspace}: A Toolbox for Manipulating and Assessing Colors and Palettes}, ## author = {Achim Zeileis and Jason C. Fisher and Kurt Hornik and Ross Ihaka and Claire D. McWhite and Paul Murrell and Reto Stauffer and Claus O. Wilke}, ## year = {2019}, ## institution = {arXiv.org E-Print Archive}, ## type = {arXiv}, ## number = {1903.06490}, ## month = {March}, ## url = {http://arxiv.org/abs/1903.06490}, ## } ## ## @Article{colorspace2009a, ## title = {Escaping {RGB}land: Selecting Colors for Statistical Graphics}, ## author = {Achim Zeileis and Kurt Hornik and Paul Murrell}, ## journal = {Computational Statistics \\&amp; Data Analysis}, ## year = {2009}, ## volume = {53}, ## number = {9}, ## pages = {3259--3270}, ## doi = {10.1016/j.csda.2008.11.033}, ## } ## ## @Article{colorspace2009b, ## title = {Somewhere over the Rainbow: How to Make Effective Use of Colors in Meteorological Visualizations}, ## author = {Reto Stauffer and Georg J. Mayr and Markus Dabernig and Achim Zeileis}, ## journal = {Bulletin of the American Meteorological Society}, ## year = {2009}, ## volume = {96}, ## number = {2}, ## pages = {203--216}, ## doi = {10.1175/BAMS-D-13-00155.1}, ## } ## ## @Article{Formula2010, ## title = {Extended Model Formulas in {R}: Multiple Parts and Multiple Responses}, ## author = {Achim Zeileis and Yves Croissant}, ## journal = {Journal of Statistical Software}, ## year = {2010}, ## volume = {34}, ## number = {1}, ## pages = {1--13}, ## doi = {10.18637/jss.v034.i01}, ## } ## ## @Book{ggplot22016, ## author = {Hadley Wickham}, ## title = {ggplot2: Elegant Graphics for Data Analysis}, ## publisher = {Springer-Verlag New York}, ## year = {2016}, ## isbn = {978-3-319-24277-4}, ## url = {https://ggplot2.tidyverse.org}, ## } ## ## @Book{knitr2015, ## title = {Dynamic Documents with {R} and knitr}, ## author = {Yihui Xie}, ## publisher = {Chapman and Hall/CRC}, ## address = {Boca Raton, Florida}, ## year = {2015}, ## edition = {2nd}, ## note = {ISBN 978-1498716963}, ## url = {https://yihui.org/knitr/}, ## } ## ## @InCollection{knitr2014, ## booktitle = {Implementing Reproducible Computational Research}, ## editor = {Victoria Stodden and Friedrich Leisch and Roger D. Peng}, ## title = {knitr: A Comprehensive Tool for Reproducible Research in {R}}, ## author = {Yihui Xie}, ## publisher = {Chapman and Hall/CRC}, ## year = {2014}, ## note = {ISBN 978-1466561595}, ## url = {http://www.crcpress.com/product/isbn/9781466561595}, ## } ## ## @Book{lattice2008, ## title = {Lattice: Multivariate Data Visualization with R}, ## author = {Deepayan Sarkar}, ## publisher = {Springer}, ## address = {New York}, ## year = {2008}, ## note = {ISBN 978-0-387-75968-5}, ## url = {http://lmdvr.r-forge.r-project.org}, ## } ## ## @Article{lubridate2011, ## title = {Dates and Times Made Easy with {lubridate}}, ## author = {Garrett Grolemund and Hadley Wickham}, ## journal = {Journal of Statistical Software}, ## year = {2011}, ## volume = {40}, ## number = {3}, ## pages = {1--25}, ## url = {http://www.jstatsoft.org/v40/i03/}, ## } ## ## @Book{plotly2020, ## author = {Carson Sievert}, ## title = {Interactive Web-Based Data Visualization with R, plotly, and shiny}, ## publisher = {Chapman and Hall/CRC}, ## year = {2020}, ## isbn = {9781138331457}, ## url = {https://plotly-r.com}, ## } ## ## @Article{report2019, ## title = {The report package for R: Ensuring the use of best practices for results reporting}, ## author = {{Makowski} and {Dominique} and {Lüdecke} and {Daniel}}, ## journal = {CRAN}, ## year = {2019}, ## note = {R package}, ## url = {https://github.com/easystats/report}, ## } ## ## @Article{reshape2007, ## author = {Hadley Wickham}, ## journal = {Journal of Statistical Software}, ## number = {12}, ## title = {Reshaping data with the reshape package}, ## url = {http://www.jstatsoft.org/v21/i12/paper}, ## volume = {21}, ## year = {2007}, ## } ## ## @Book{rmarkdown2018, ## title = {R Markdown: The Definitive Guide}, ## author = {Yihui Xie and J.J. Allaire and Garrett Grolemund}, ## publisher = {Chapman and Hall/CRC}, ## address = {Boca Raton, Florida}, ## year = {2018}, ## note = {ISBN 9781138359338}, ## url = {https://bookdown.org/yihui/rmarkdown}, ## } ## ## @Book{rmarkdown2020, ## title = {R Markdown Cookbook}, ## author = {Yihui Xie and Christophe Dervieux and Emily Riederer}, ## publisher = {Chapman and Hall/CRC}, ## address = {Boca Raton, Florida}, ## year = {2020}, ## note = {ISBN 9780367563837}, ## url = {https://bookdown.org/yihui/rmarkdown-cookbook}, ## } ## ## @Book{survival-book, ## title = {Modeling Survival Data: Extending the {C}ox Model}, ## author = {{Terry M. Therneau} and {Patricia M. Grambsch}}, ## year = {2000}, ## publisher = {Springer}, ## address = {New York}, ## isbn = {0-387-98784-3}, ## } ## ## @Manual{textclean2018, ## title = {{textclean}: Text Cleaning Tools}, ## author = {Tyler W. Rinker}, ## address = {Buffalo, New York}, ## note = {version 0.9.3}, ## year = {2018}, ## url = {https://github.com/trinker/textclean}, ## } ## ## @Article{tidytext2016, ## title = {tidytext: Text Mining and Analysis Using Tidy Data Principles in R}, ## author = {Julia Silge and David Robinson}, ## doi = {10.21105/joss.00037}, ## url = {http://dx.doi.org/10.21105/joss.00037}, ## year = {2016}, ## publisher = {The Open Journal}, ## volume = {1}, ## number = {3}, ## journal = {JOSS}, ## } ## ## @Article{tidyverse2019, ## title = {Welcome to the {tidyverse}}, ## author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D&#39;Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani}, ## year = {2019}, ## journal = {Journal of Open Source Software}, ## volume = {4}, ## number = {43}, ## pages = {1686}, ## doi = {10.21105/joss.01686}, ## } ## ## @Article{tm2008, ## title = {Text Mining Infrastructure in R}, ## author = {Ingo Feinerer and Kurt Hornik and David Meyer}, ## year = {2008}, ## journal = {Journal of Statistical Software}, ## volume = {25}, ## number = {5}, ## pages = {1--54}, ## url = {http://www.jstatsoft.org/v25/i05/}, ## month = {March}, ## } ## ## @Article{zoo2005, ## title = {zoo: S3 Infrastructure for Regular and Irregular Time Series}, ## author = {Achim Zeileis and Gabor Grothendieck}, ## journal = {Journal of Statistical Software}, ## year = {2005}, ## volume = {14}, ## number = {6}, ## pages = {1--27}, ## doi = {10.18637/jss.v014.i06}, ## } "]]
