# Token Analysis {#tokens}

[In-Progress]

```{r TA_setup}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = TRUE)
# set to TRUE to run this on only one reference file
debugging <- FALSE
# this will expect the file or files to be in a subdirectory with the following name
refsource <- "MungingProj2"
dataDir <- "Proj2Data"
workingDir <- refsource

# prefixes for all File reads and writes
# titles for tables
titletext <- "RedditCrypto"
srs = c("CryptoCurrency","CryptoMarkets")

`%notin%` <- Negate(`%in%`)
```

## Import

```{r import_submissions, include = FALSE}
TknsS<- read.csv(paste0(dataDir, "/SubmTkns.csv")) 
TknsC<- read.csv(paste0(dataDir, "/CommTkns.csv"))
```

## Token Counts

Token counts are done at several different levels. The first being word frequency within each subreddit, which allows for a comparison of common words across communitas. The second is word frequency within each subreddit page (i.e. "Top", "Controversial", "Hot"), which allows for a comparison of common words across strata within each communitas.

```{r count}
allCount <-TknsC %>%
    group_by(subreddit) %>% #group words by affiliation label
    count(word, sort = TRUE) %>%
    ungroup()
```


### Most Frequent Tokens

The following code show the top 25 most frequently occuring words within each subreddit.

```{r top_25}
sr_all_n25 <- allCount %>% 
  group_by(subreddit) %>%
  top_n(25) %>%
  ungroup()
```

## Wordclouds

Wordclouds are then constructed using the top 50 words from each subreddit page.

### All words (Submissions and Comments)
```{r wc_allCount}
#sr_allCount <- allCount %>% filter(!subreddit == "r/all [control]")
sr_allWC <- allCount %>%
  top_n(100) %>%
  mutate(prop = n / max(n))

color=diverge_hcl(length(sr_allWC$prop))[rank(sr_allWC$prop)]
```


```{r wordcloud1, warnings = FALSE, messages = FALSE}
set.seed(29)
ggplot(sr_allWC, aes(label = word, size = prop, color = prop)) +
    geom_text_wordcloud_area(shape = 'circle', rm_outside = TRUE) +
    scale_size_area(max_size = 30) +
    #scale_colour_gradient2(low = "black", mid = "red4", high = "red", space = "Lab", aesthetics = "color") +
    theme_minimal()

ggsave("CC_wordcloud.pdf", device = "pdf", path = "CCViz", height = 10, width = 16)
```


TF_IDF is a tool used to show distinct word usage across groups by taking the difference between the term frequency `tf` and inverse document frequency `idf`, ultimately revealing the tokens that are most unique to each subreddit.

```{r Onegram tf-idf }
#Create TF-IDF matrix for Controversial Submissions
allIDF <- allCount %>%
  bind_tf_idf(word, subreddit, n) %>% #construct tf_idf by affiliation label
  arrange(desc(tf_idf)) %>% 
  mutate(word = factor(word, levels = rev(unique(word))))
```


## TF-IDF

The below figure shows that the religious communitas can be identified through the symbols presented in their distinguishing tokens. This methodology is useful for exploring the divergent symbolic identities of collective consciousnesses. The tf-ifd analysis below presents us with sets of names, concepts, and phrases that reference the symbolic systems of each religious identity. 

```{r all tf_idf}
allIDF %>%
  group_by(subreddit) %>% 
  top_n(10) %>% 
  ungroup()%>%
  ggplot(aes(word, tf_idf, fill = subreddit)) + geom_col(show.legend = FALSE) + labs(x = NULL, y = "tf-idf") + facet_wrap(~subreddit, ncol = 2, scales = "free_y") + coord_flip() + theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggsave("tf_idf.pdf", device = "pdf", path = "CCViz", height = 8, width = 6)
```




## Posts over time

```{r split_dates}
TknsByDate <- TknsC %>%
  separate(created, c("created", "time"), " ") %>%
  mutate(created = ymd(created)) %>%
  mutate_at(vars(created), funs(year, month, day))
```

```{r month_count}
TknsByMonth <-TknsByDate %>%
    filter(year > 2015) %>%
    mutate(Month = make_date(year, month))

monthCount <- TknsByMonth %>%
    group_by(subreddit, month, year) %>% #group words by affiliation label
    count(word, sort = TRUE) %>% #count and create column 'n'
    top_n(10, n) %>%
    ungroup()

monthCount <- monthCount %>%
    group_by(subreddit, month, year) %>%
    mutate(prop = n / max(n))
    
table(monthCount$subreddit)
```

```{r}
ggplot(monthCount, aes(
  label = word,
  size = prop,
  color = prop
)) +
  geom_text_wordcloud_area(rm_outside = TRUE) +
  scale_size_area(max_size = 20) +
  #scale_colour_gradient2(low = "black", mid = "red4", high = "red", space = "Lab", aesthetics = "color") +
  theme_minimal() +
  facet_grid(vars(month), vars(year))

ggsave(
  "CC_timeline.pdf",
  device = "pdf",
  path = "CCViz",
  height = 30,
  width = 20
)
```


## Currencies

```{r}
BTC <- c("Bitcoin", "bitcoin", "BTC", "btc", "Btc")
ETH <- c("Ethereum", "ethereum", "ETH", "eth", "Eth")
XRP <- c("Ripple", "ripple", "XRP", "xrp", "Xrp")
LTC <- c("Litecoin", "litecoin", "LTC", "ltc", "Ltc")

currencies <- c(BTC, ETH, XRP, LTC, LINK)

# For frequency analysis
CurTkns <- TknsByDate %>%
  filter(word %in% currencies)

# For sentiment analysis
CurComms <- CommData %>%
  filter(comm_id %in% CurTkns$comm_id)
```

### Assigning Identifiers

```{r}
CurTkns <- CurTkns %>%
  mutate(Coin = case_when(
    .$word %in% BTC ~ "BTC",
    .$word %in% ETH ~ "ETH",
    .$word %in% XRP ~ "XRP",
    .$word %in% LTC ~ "LTC",
    .$word %in% LINK ~ "LINK"
  ))
```


```{r}
# Establish date column for grouping
curTknsByMonth <-CurTkns %>%
    mutate(Month = make_date(year, month))

curCounts <- curTknsByMonth %>%
    group_by(Coin) %>% #group words by affiliation label
    count(word, sort = TRUE) %>% #count and create column 'n'
    ungroup()

curCountsbyMonth <- curTknsByMonth %>%
    group_by(Coin, Month) %>% #group words by affiliation label
    count(word, sort = TRUE) %>% #count and create column 'n'
    ungroup()
```

```{r}
ggplot(curCounts) +
  geom_bar(aes(x = Coin, y = n, fill = Coin), stat = "identity")
```


```{r}
ggplot(curCountsbyMonth) +
  geom_bar(aes(x = Coin, y = n, fill = Coin), stat = "identity") +
  facet_wrap(~Month)
```

```{r}
write.csv(curTknsByMonth, paste0(dataDir, "/CoinTknsMonthly.csv"), row.names = FALSE)
```

