--- 
title: "An analysis of Cryptocurrency"
author: "Kalani Stanton, Timothy McCormack, Carolyn Herrera, Wei Chun Tien, "
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
output_dir: "docs"
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes~
github-repo: KalaniStanton/MungingProj2
description: ""
---
--- 
title: "An analysis of Cryptocurrency"
author: "Kalani Stanton, Timothy McCormack, Carolyn Herrera, Wei Chun Tien, "
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
output_dir: "docs"
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes~
github-repo: KalaniStanton/MungingProj2
description: ""
---

# Preface {-}

This project is an exploration of language in the comments from the two most popular subreddits oriented towards the discussion of Crypto-currencies and the trading of block-chain assets. The methodologies used herein include tokenization, sentiment analysis using dictionaries, and associative visualization primarily using packages found within the tidyverse.

<!--chapter:end:index.Rmd-->


# Introduction {#intro}

Placeholder



<!--chapter:end:01-intro.Rmd-->

# Cleaning and Tokenization {#clean}

```{r clean_setup}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = TRUE)
# set to TRUE to run this on only one reference file
debugging <- FALSE
# this will expect the file or files to be in a subdirectory with the following name
refsource <- "MungingProj2"
dataDir <- "Proj2Data"
workingDir <- refsource

# prefixes for all File reads and writes
# titles for tables
titletext <- "RedditCrypto"
srs = c("CryptoCurrency","CryptoMarkets")

`%notin%` <- Negate(`%in%`)
```

## Import Clean

The data imported in the following code chunk was collected using the Python Reddit API Wrapper to extract data from the subreddits in this study 

```{r}
subm_fnames <- list.files(dataDir, pattern = "*_submissions.csv", full.names = TRUE)
comm_fnames <- list.files(dataDir, pattern = "*_comments.csv", full.names = TRUE)
subr_fnames <- list.files(dataDir, pattern = "*_subreddit.csv", full.names = TRUE)

for (i in 1:length(subm_fnames)) 
  assign(srs[i], read.csv(subm_fnames[i]))
SubmData<- rbind(CryptoCurrency, CryptoMarkets)
for (i in 1:length(comm_fnames)) 
  assign(srs[i], read.csv(comm_fnames[i]))
CommData<- rbind(CryptoCurrency, CryptoMarkets)
for (i in 1:length(subr_fnames)) 
  assign(srs[i], read.csv(subr_fnames[i]))
SubrData<- rbind(CryptoCurrency, CryptoMarkets)
```

### Fix Names

Subreddits are identifiable among online communities in that they are often referenced according to the url suffix of the subreddit webpage (i.e. r/CryptoCurrency comes from reddit.com/r/CryptoCurrency). To retain this cultural identifier, we modify the names

```{r}
srs <- unique(SubmData$subreddit)

SubmData <- SubmData %>%
    mutate(
      subreddit = case_when(
        .$subreddit == srs[1] ~ "r/CryptoCurrency",
        .$subreddit == srs[2] ~ "r/CryptoMarkets")
      ) %>%
  ungroup()

srs <- unique(CommData$subreddit)

CommData <- CommData %>%
    mutate(
      subreddit = case_when(
        .$subreddit == srs[1] ~ "r/CryptoCurrency",
        .$subreddit == srs[2] ~ "r/CryptoMarkets")
      ) %>%
  ungroup()
```

### Subreddit information
```{r}
submnums <- table(SubmData$subreddit)
SubmNums <- as.data.frame(submnums, .name_repair = "minimal")
colnames(SubmNums)[1] <- "Subreddit"
ggplot(SubmNums, aes(x = Subreddit, y = Freq, fill = Subreddit)) + geom_bar(stat = "identity") + scale_y_continuous(name="# of Submissions by Subreddit", labels = scales::comma)
```

```{r}
c <- CommData

commnums <- table(c$subreddit)
CommNums <- as.data.frame(commnums, .name_repair = "minimal")
colnames(CommNums)[1] <- "Subreddit"
ggplot(CommNums, aes(x = Subreddit, y = Freq, fill = Subreddit)) + geom_bar(stat = "identity") + scale_y_continuous(name="# of Comments by Subreddit (from 300 posts each)", labels = scales::comma)
```

```{r, include = FALSE}

#r_data <- cbind(Subreddits[-c(8)], r_data)
#rownames(r_data) <- Subreddits[-c(8)]
#colnames(r_data)[c(3,4,7)] <- c("Title", "No. of Subscribers", "No. of #Comments")
```

```{r}
kable(SubrData) %>%
  kable_styling("striped", full_width = F)
```


*Submissions* from each subreddit:

```{r, include = FALSE}
table(SubmData$subreddit)
```

*Comments* in each subreddit:

```{r, include = FALSE}
table(CommData$subreddit)
```

## Submissions

The analysis of langauge begins by quantifying the presence of words in each Subm, through the process of *tokenization*. *Tokens* are discrete strings of words or characters that can be isolated as n-grams; with *n* pertaining to the number of words in each token. Tokens are pulled from the body of text that is most informative for the purposes of analysis. For submissions, the informative text is the *title* of the submission, which contains information on topics; whereas, for comments, the informative text is the `comment` itself. 

### Clean

To ensure that our data accurately represent activity within the communitas, we want to ensure that each observation is a unique instance of engagement. Reposts tend to be common on reddit, so using a `distinct()` function on the text column will remove any duplicate posts. Additionally, any posts that are removed from the subreddits return  an `NA` value in `user` column, thus we can remove deleted comments by filtering out all non `user == NA`.

```{r clean submissions}
#Clean
Encoding(SubmData$text) <- "UTF-8"

Submissions <- SubmData %>%
  group_by(user) %>%
    filter(!is.na(user)) %>% # Take out deleted comments
    ungroup() %>%
    distinct(text, .keep_all = TRUE) #remove duplicate submissions

paste("Removed", nrow(SubmData) - nrow(Submissions),"submissions.")
```


```{r tokenize_submissions}
data(stop_words)

SubmissionTkns <- Submissions %>%
  group_by(subreddit) %>%
    unnest_tokens(word, text) %>%
    ungroup()

```


As part of the tokenization process for submissions, we remove stop words (e.g. "and", "a", "the") because we are interested in using our tokens to identify prevalent topics of discussion and attitudes in the subreddits.

#### Tokens

```{r}
#Create object for numbers, so that we can remove them from the data
nums <- SubmissionTkns %>% filter(str_detect(word, "^[0-9]")) %>% select(word) %>% unique() #Source: https://richpauloo.github.io/2017-12-29-Using-tidytext-to-make-word-clouds/

SubmissionTkns <- SubmissionTkns %>%
    anti_join(stop_words, by = "word") %>%
    anti_join(nums, by = "word") %>%
    filter(!grepl("_", .$word))

G1Subm <- SubmissionTkns

```

## Comments 

### Clean

```{r}
Encoding(CommData$text) <- "UTF-8"

Comments <- CommData %>%
  group_by(user) %>%
    filter(!is.na(user)) %>% # Take out deleted comments
    ungroup() %>%
    distinct(text, .keep_all = TRUE) #remove duplicate comments

paste("Removed", nrow(CommData) - nrow(Comments),"comments")
```


```{r rm urls}
url_regex <- "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"

Comments$text <-stringi::stri_trans_general(Comments$text, "latin-ascii") #converts text encoding, otherwise the tokenizer won't retain contractions

Comments$text <- str_remove_all(Comments$text, url_regex)
```

#### Tokenize

```{r}
#tokenize
G1Comm <- Comments %>%
  unnest_tokens(word, text)
```


```{r}
#Create object for numbers, so that we can remove them from the data
nums <- G1Comm %>% filter(str_detect(word, "^[0-9]")) %>% select(word) %>% unique() #Source: https://richpauloo.github.io/2017-12-29-Using-tidytext-to-make-word-clouds/

Bots <- filter(G1Comm, word == "bot")

data(stop_words)

CommTkns <- G1Comm %>%
    anti_join(stop_words) %>% #take out stop words (i.e. 'a', 'an', 'of', 'the')
    anti_join(nums, by = "word") %>%
    filter(user %notin% Bots$user) %>%
    filter(!grepl("_", .$word))
    #ungroup()

G1Comm <- CommTkns

paste("Removed", nrow(CommData) - nrow(Comments),"tokens")
```

## Export

```{r export_submissions}
write.csv(SubmissionTkns, paste0(dataDir, "/SubmTkns.csv"), row.names = FALSE)
```

```{r export_comments}
write.csv(CommTkns, paste0(dataDir, "/CommTkns.csv"), row.names = FALSE)
```


<!--chapter:end:02-Clean.Rmd-->


# Token Analysis {#token-analysis}

Placeholder


## Import
## Token Counts
### Most Frequent Tokens
## Wordclouds
### All words (Submissions and Comments)
## Posts over time
## Currencies
### Assigning Identifiers

<!--chapter:end:03-TokenAnalysis.Rmd-->


# Sentiment Analysis {#sentiments}

Placeholder


## Data Shaping
### `get_sentiments()`
## Price & Sentiment over Time
### BTC
### ETH
### LTC
### XRP
## User Sentiment
### Angriest
### Happiest
### Saddest
## Sentiment Distribution By Coin
## Descriptive Statistics
## Most Abundant Sentiment Over Time
## Plots of Top Sentiment Over Time
## TF-IDF

<!--chapter:end:04-SentimentAnalysis.Rmd-->

# Concluding Remarks {#conclusions}

## Tim McCormack

What I learned most about from this project is the power of R when it comes to qualitative analysis. The plethora of Application Programming Interfaces availabe via the R platform yields an awesome power to Data Science enthusiasts. This project has also opened my eyes to the possibilities of new data sources. With the confidence of API programming in R, the entire web becomes a potential field for data collection. This project also gave me an appreciation for how a small company operates, all of the team members wore many hats. My contributions included researching API's for market data, building functions in R to partition data frames, and merging files that contained multiple team members work. In the future I hope that I may use this project to educate Data Science comrades about the potential of Sentiment Analysis in R.

## Wei Chun Tien

<!--chapter:end:05-Comments.Rmd-->

`r if (knitr:::is_html_output()) '
# References {-}

'`
```{r, messages = FALSE, warning=FALSE}
#install.packages("devtools")
#devtools::install_github("neuropsychology/report")
library(report) #Package for citing
```

## Packages Used

```{r include=TRUE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown','packages.bib', "book.bib"))
```

<!--chapter:end:06-references.Rmd-->

